---
title: "Fold change example"
author: "Will Landau, Luwis Diya"
output: html_document
bibliography: bibliography.bib
---

```{r setup, include = FALSE}
library(coda)
library(rjags)
library(brms)
library(tidyverse)
library(broom.mixed)
knitr::opts_chunk$set(echo = TRUE)
set.seed(0)
```

## Background

In its simplest form, a Bayesian workflow typically has two steps:

1. Estimate the posterior distribution for a model given the data.
2. Derive interpretable and scientifically meaningful summary statistics from that posterior.

For a fixed observed dataset $y$ and uncertain model parameters $\theta$, recall the posterior density:

$$
p(\theta | y) = \frac{p(y | \theta)p(\theta)}{p(y)}
$$
Usually, the prior predictive density $p(y)$ is too difficult to estimate, but the data model $p(y | \theta)$ and the prior density $p(\theta)$ are simple. So to estimate the posterior $p(\theta | y)$, we use numerical methods that ignore $p(y)$ and only require $p(y | \theta)$ and  $p(\theta)$.

Markov chain Monte Carlo (MCMC) is a popular family of such methods (although others exist too). In MCMC, we run one or more Markov chains whose target distribution is the posterior. Run for a total of $M < \infty$ discrete iterations, these Markov chains produce $M$ parameter samples $\theta^{(1)}, \ldots, \theta^{(M)}$ from $p(\theta | y)$. We use these samples to derive interpretable and scientifically meaningful summary statistics from that posterior. For example, for each scalar element $\theta_i$ of the parameter vector $\theta \in \mathbb{R}^I$,

$$
\begin{aligned}
E(\theta_i) \approx \frac{1}{M} \sum_{m = 1}^M \theta_i^{(m)}
\end{aligned}
$$

In fact, if $g: \mathbb{R}^I \mapsto \mathbb{R}$ is any function on the parameter space, we can easily estimate the expectation of $g(\theta)$.

$$
\begin{aligned}
E \left (g(\theta) \right ) \approx \frac{1}{M} \sum_{m = 1}^M g \left (\theta^{(m)} \right )
\end{aligned}
$$

Likewise, medians and other quantiles of $g(\theta)$ easily follow from transformed posterior samples $g \left (\theta^{(1)} \right ), \ldots, g \left (\theta^{(M)} \right )$. No matter how complicated $g()$ may be, these these summary statistics do not require any Jacobian adjustment. By contrast, frequentist models may require the delta method, Fieller's method, or other specialized derivation to properly estimate scientifically meaningful quantities from a fitted model.

This tutorial demonstrates how convenient and useful the Bayesian paradigm can be in these situations. In the example below, the scientifically meaningful quantity of interest is a ratio of means. A simple frequentist model needs an extra derivation to estimate this ratio, whereas equivalent Bayesian models can estimate it directly.

## Fold change example

Consider a pre-clinical animal study (mice) with one treatment group and one placebo group. The outcome measure is qPCR gene expression on the log scale, and the goal is to estimate fold change of the outcome for treatment versus placebo. This tutorial compares a simple frequentist model and the analogous Bayesian model in this analysis. In each case, the fold change is not an explicit parameter in the model and needs to be calculated by transforming other model parameters.

## Data

We simulate 10 mice from a normal distribution for each of the two groups.

Quantity | placebo | Treatment
---|---|---
Mean | 3 | 4
Standard deviation | 1 | 1

In our dataset, the `y` column is the outcome we are modeling, and the `i` column is the group designation ($i = 1$ for placebo, $i = 2$ for treatment).

```{r}
library(tidyverse)
placebo <- tibble(
  y = rnorm(n = 10, mean = 3, sd = 1),
  i = 1
)
treatment <- tibble(
  y = rnorm(n = 10, mean = 4, sd = 1),
  i = 2
)
data <- bind_rows(placebo, treatment)
data
```

## Frequentist analysis

The frequentist model is a simple cell means model with one mean for each group. Below, $i = 1, \ldots, I$ is the group designation, and $j = 1, \ldots, J$ is the index of the mouse within each group. Our dataset has $I = 2$ gruops and $J = 10$ mice per group. $y_{ij}$ is the observed log-scale qPCR expression level of mouse $j$ in group $i$, and $\mu_i$ is the unknown mean parameter of group $i$. $\sigma$ is the unknown residual standard deviation, and $\stackrel{\text{ind}}{\sim}$ is shorthand to indicate independent and identically distributed quantities.

$$
\begin{aligned}
y_{ij} \stackrel{\text{ind}}{\sim} \text{Normal}(\mu_i, \sigma^2)
\end{aligned}
$$



The joint density of $y = (y_{11}, \ldots, y_{IJ})$ given the parameters can be written as

$$
p_{\mu_1, \mu_2, \sigma}(y) = \prod_{j = 1}^{J} \frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{1}{2} \left (\frac{y_{ij} - \mu_i}{\sigma} \right )^2}
$$

with data model $p(y | \mu_1, \mu_2, \sigma) = p_{\mu_1, \mu_2, \sigma}(y)$. The fold change we want to estimate is the ratio of the treatment group mean to that of the placebo:

$$
\text{Fold change} = \frac{\mu_2}{\mu_1}
$$

We fit the model using `lm()` in R and note the estimate and standard error of each mean (placebo and treatment).

```{r}
fit_frequentist <- lm(y ~ 0 + i, data = mutate(data, i = ordered(i))) %>%
  broom::tidy() %>%
  rename(standard_error = std.error) %>%
  mutate(across(-all_of("term"), round, digits = 3)) %>%
  select(term, estimate, standard_error)

fit_frequentist
```

To estimate the fold change in the frequentist paradigm, we cannot simply take the ratio of the maximum likelihood estimates in the table above. To properly characterize the new estimate and its uncertainty, we need a formal change of variables. The delta method ([@beyene2005]) is one such technique, and it allows us to approximate the mean and 95% confidence interval of the fold change $\mu_2/\mu_1$. The implementation below assumes independence between $\mu_1$ and $\mu_2$, which is consistent with the underlying experimental conditions.

```{r}
delta_method <- function(
  estimate_treatment,
  estimate_placebo,
  standard_error_treatment,
  standard_error_placebo,
  alpha = 0.05
) {
  a <- estimate_treatment
  b <- estimate_placebo
  se_a <- standard_error_treatment
  se_b <- standard_error_placebo
  theta <- a / b
  v11 <- se_a ^ 2
  v22 <- se_b ^ 2
  z <- qnorm(p = alpha / 2, lower.tail = FALSE)
  sigma <- sqrt((1 / (b ^ 2)) * (v11 + (theta ^ 2) * v22))
  lower <- theta - z * sigma
  upper <- theta + z * sigma
  tibble(estimate = theta, lower = lower, upper = upper)
}

index_treatment <- which(fit_frequentist$term == "i2")
index_placebo <- which(fit_frequentist$term == "i1")

delta_approximation <- delta_method(
  estimate_treatment = fit_frequentist$estimate[index_treatment],
  estimate_placebo = fit_frequentist$estimate[index_placebo],
  standard_error_treatment = fit_frequentist$standard_error[index_treatment],
  standard_error_placebo = fit_frequentist$standard_error[index_placebo],
  alpha = 0.05
)

delta_approximation
```

We can alternatively use Fieller's method ([@fieller1940], [@cordell1999], [@beyene2005]) to approximate the same quantities. The implementation below also assumes independence between $\mu_1$ and $\mu_2$, but it can be generalized to data where these quantities may be correlated.

```{r}
fieller_method <- function(
  estimate_treatment,
  estimate_placebo,
  standard_error_treatment,
  standard_error_placebo,
  alpha = 0.05
) {
  a <- estimate_treatment
  b <- estimate_placebo
  se_a <- standard_error_treatment
  se_b <- standard_error_placebo
  theta <- a / b
  v11 <- se_a ^ 2
  v22 <- se_b ^ 2
  z <- qnorm(p = alpha / 2, lower.tail = FALSE)
  k <- (z ^ 2 * v22) / b ^ 2
  estimate <- theta + (k / (1 - k)) * theta
  margin <- z / (b * (1 - k)) * sqrt(v11 + theta ^ 2 * v22 - k * v11)
  bound1 <- estimate - margin
  bound2 <- estimate + margin
  lower <- pmin(bound1, bound2)
  upper <- pmax(bound1, bound2)
  tibble(estimate = estimate, lower = lower, upper = upper)
}

fieller_approximation <- fieller_method(
  estimate_treatment = fit_frequentist$estimate[index_treatment],
  estimate_placebo = fit_frequentist$estimate[index_placebo],
  standard_error_treatment = fit_frequentist$standard_error[index_treatment],
  standard_error_placebo = fit_frequentist$standard_error[index_placebo],
  alpha = 0.05
)

fieller_approximation
```

## Bayesian model

The Bayesian model is the same as the frequentist one except

1. We treat the dataset as fixed and known, and we treat the parameters as unknown and uncertain.
2. We assign independent prior distributions to each of the scalar parameters $\mu_1$, $\mu_2$, and $\sigma$. Together, they constitute and overall joint prior on the parameter space of the model.

$$
\begin{aligned}
&y_{ij} \stackrel{\text{ind}}{\sim} \text{Normal}(\mu_i, \sigma^2) \\
&\qquad \mu_i \stackrel{\text{ind}}{\sim} \text{Normal}(0, s_\mu^2) \\
&\qquad \sigma \sim \text{Uniform}(0, s_\sigma)
\end{aligned}
$$

Above, $s_\mu$ and $s_\sigma$ are constant hyperparameters that placebo the diffuseness of the priors. If we lack prior information, diffuse priors (e.g. large values of $s_\mu$ and $s_\sigma$) may be appropriate if this is computationally feasible.

## Posterior density

If we were to implement Markov chain Monte Carlo (MCMC) from scratch, we would need to derive the posterior distribution up to a proportionality constant. Our parameter vector $\theta$ is $(\mu_1, \mu_2, \sigma)$. We already identified the data model $p(y | \theta) = p(y | \mu_1, \mu_2, \sigma)$ in the frequentist section above. The prior $p(\theta)$ is $p(\mu_1, \mu_2, \sigma)$, which factors into a product $p(\mu_1) p(\mu_2) p(\sigma)$ because of the independence assumptions of the model. $p(\mu_1)$ and $p(\mu_2)$ are normal densities, and $p(\sigma)$ is a uniform density. With all these pieces, we can derive the posterior density up to a proportionality constant.

$$
\begin{aligned}
p(\theta | y) &\propto p(y | \theta) p(\theta) \\
&= p(y | \mu_1, \mu_2, \sigma) p(\mu_1, \mu_2, \sigma) \\
&= p(y | \mu_1, \mu_2, \sigma) p(\mu_1) p(\mu_2) p(\sigma) \\
&= \left [ \prod_{j = 1}^{J} \frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{1}{2} \left (\frac{y_{ij} - \mu_i}{\sigma} \right )^2} \right ] \frac{1}{2 \pi s_\mu} e^{-\frac{1}{2} \left ( \frac{\mu_1}{s_\mu} \right )^2} \frac{1}{2 \pi s_\mu} e^{-\frac{1}{2} \left ( \frac{\mu_2}{s_\mu} \right )^2} I \left (0 < \sigma < s_\sigma \right )
\end{aligned}
$$

## Bayesian software

At this point, there are a variety of MCMC methods to estimate the posterior. Examples include rejection sampling, Metropolis-Hastings, slice sampling, Gibbs sampling, and Hamiltonian Monte Carlo. To implement one of these algorithms from scratch, you would need to work directly with the expression above. This is a worthwhile approach for complicated models you are willing to spend several months developing and optimizing, both the statistics and the code.

However, there are already several existing software packages that can run MCMC on most Bayesian models. In these packages, you do not even have to derive the posterior distribution up to a proportionality constant. You merely need to identify the components of the likelihood and prior using the simple syntax supported by the tool. Examples of such tools are JAGS, `brms`, Stan, NIMBLE, `greta`, OpenBUGS, and WinBUGS. This tutorial will focus on JAGS, which uses Gibbs sampling, and `brms`, a convenient layer on top of Stan for Bayesian regression models. 

## JAGS

JAGS stands for Just Another Gibbs Sampler. It supports a special R-like syntax to write a model in terms of distributions on the data and the parameters. Given a user-supplied model definition, JAGS runs Gibbs sampling and returns posterior samples for the model. We express our qPCR model in the JAGS model code below.

```{r}
jags_code <- "
model{
  for (n in 1:N) {
    y[n] ~ dnorm(mu[i[n]], (1 / (sigma * sigma))) # dnorm() accepts precision as the second argument.
  }
  for (j in 1:I) {
    mu[j] ~ dnorm(0, (1 / (s_mu * s_mu)))
  }
  sigma ~ dunif(0, s_sigma)
}
"
```

We will run JAGS using R. In the R console, we convert our dataset into a a list format that JAGS can understand.

```{r}
library(rjags)
jags_data <- list(
  y = data$y,
  N = length(data$y),
  i = data$i,
  I = length(unique(data$i)),
  s_mu = 10,
  s_sigma = 10
)
str(jags_data)
```

We create a JAGS model object using the `jags.model()` function. This object brings the data and model definition together, interprets the model, and tunes the Gibbs sampler in an adaptation phase (in this case, 2000 iterations). In addition, we use 4 Markov chains that begin at different starting values. Running multiple chains allows us to use diagnostics that can uncover potential problems with the MCMC (more on that later).

```{r, output = FALSE, message = FALSE}
jags_model <- rjags::jags.model(
  file = textConnection(jags_code),
  data = jags_data,
  n.chains = 4,
  n.adapt = 2000
)
```

After adaptation, we stop tuning and run the MCMC a few thousand more iterations to allow each Markov chain to coverge to the posterior distribution. This is the "burn-in" or "warmup" phase. After it is complete, we discard earlier iterations and only keep track of the current state of each Markov chain.

```{r, output = FALSE, message = FALSE}
update(jags_model, n.iter = 2000, quiet = TRUE)
```

After warmup, we hope the Markov chains have converged to the poseterior. It is impossible to prove convergence in the general case, but there are trusted diagnostics to check for obvious issues, and the next section will describe them. For now, let us continue the Markov chains and save the resulting MCMC samples this time.

```{r, output = FALSE, message = FALSE}
coda <- rjags::coda.samples(
  model = jags_model,
  variable.names = c("mu", "sigma"),
  n.iter = 4000
)
samples <- tibble::as_tibble(posterior::as_draws_df(coda))
```

We now have a nice tidy data frame with one row per MCMC sample and a column for each parameter. In addition, the `.chain` column identifies the Markov chain that produced each MCMC sample. The `.iteration` column indexes each sample within its Markov chain, and the `.draw` column indexes each sample overall. We ran each Markov chain for 4000 iterations after warmup, and there wre 4 chains, so we have a total of 16000 samples.

```{r, paged.print = FALSE}
samples
```

## Bayesian estimation of fold change

To estimate the fold change, we simply use the MCMC samples. First, we apply the transformation $g(\mu_1, \mu_2, \sigma) = \mu_2 / \mu_1$ derive posterior samples of the fold change.

```{r}
samples_fold_change <- samples$`mu[2]` / samples$`mu[1]`

str(samples_fold_change)
```

Next, we use the transformed samples to estimate the mean, 2.5th percentile, and 97.5th percentile. The two percentiles are the respective lower and upper bound of an equal-tailed 95% posterior interval on the fold change $\mu_2/\mu_1$.

```{r}
jags_estimate <- tibble(
  estimate = mean(samples_fold_change),
  lower = quantile(samples_fold_change, prob = 0.025),
  upper = quantile(samples_fold_change, prob = 0.975)
)

jags_estimate
```

For this model, the Bayesian model gives us the same results as the frequentist model, but with less mathematical work. Extensions of this model could make use of prior information to inform the prior distributions on the parameters, which could lead to slightly different and more precise estimates of the fold change.

```{r}
estimates <- bind_rows(
  delta = delta_approximation,
  fieller = fieller_approximation,
  jags = jags_estimate,
  .id = "method"
)
estimates
```

```{r}
library(ggplot2)
ggplot(estimates) +
  geom_point(aes(x = method, y = estimate, color = method)) +
  geom_errorbar(aes(x = method, ymin = lower, ymax = upper, color = method)) +
  expand_limits(y = 0)
```

## Informative priors

So far, we have been using diffuse priors for $\mu_1$ and $\mu_2$. The goal for the priors was to be "non-informative", i.e. to reduce the influence of the prior distribution as much as possible and let the data drive the results. But in some situations, e.g. for computational reasons or to make us of historical information, it is desirable to use more informative priors. In cases like these, it is useful to run sensitivity analyses to understand the effect of the prior on the marginal posterior of the quantity of interest. Below, we experiment with a grid of $r_1$ and $r_2$ values and show the resulting posterior interval of $\mu_2/\mu_1$ for each analysis. For this case, suppose we choose prior means $m_1 = m_2 = 3$ based on historical data or expert elicitation. Low $r_i$ values impose a tight induced prior for $\mu_2/\mu_1$ centered at 1 (lower variance at the cost of increase bias towards a lack of differential expression).

```{r}
single_analysis <- function(
  data,
  jags_file,
  m1 = 0,
  m2 = 0,
  r1 = 10,
  r2 = 10
) {
  jags_data <- list(
    y = data$y,
    i = data$i,
    N = nrow(data),
    I = length(unique(data$i)),
    m = c(m1, m2),
    r = c(r1, r2),
    s = 10
  )
  n_chains <- 4
  n_adapt <- 2e3
  n_warmup <- 2e3
  n_iterations <- 4e3
  inits <- replicate(
    n_chains,
    list(
      ".RNG.name" = "base::Mersenne-Twister",
      ".RNG.seed" = sample.int(n = 1e6, size = 1)
    ),
    simplify = FALSE
  )
  model <- rjags::jags.model(
    file = jags_file,
    data = jags_data,
    n.chains = n_chains,
    n.adapt = n_adapt,
    inits = inits
  )
  update(model, n.iter = n_warmup, quiet = quiet)
  coda <- rjags::coda.samples(
    model = model,
    variable.names = c("mu", "sigma"),
    n.iter = n_iterations
  )
  samples <- tibble::as_tibble(posterior::as_draws_df(coda))
  samples_fold_change <- samples$`mu[2]` / samples$`mu[1]`
  summary <- posterior::summarise_draws(samples)
  tibble::tibble(
    estimate = mean(samples_fold_change),
    lower = quantile(samples_fold_change, prob = 0.025),
    upper = quantile(samples_fold_change, prob = 0.975),
    m1 = m1,
    m2 = m2,
    r1 = r1,
    r2 = r2,
    max_rhat = max(summary$rhat),
    min_ess_bulk = min(summary$ess_bulk),
    min_ess_tail = min(summary$ess_tail)
  )
}
```

```{r, include = FALSE}
library(purrr)
out <- map_dfr(
  .x = seq_len(20) / 10,
  ~single_analysis(
    data = data,
    jags_file = jags_file,
    m1 = 3,
    m2 = 3,
    r1 = .x,
    r2 = .x
  )
)
```

Below, we plot the marginal posterior mean and 95% posterior interval of $\mu_2/mu_1$. Each interval corresponds to an independent Bayesian analysis with a different value of $r_1$. As expected, we see a narrow interval centered at 1 for small $r_1$ and $r_2$ and a wider interval shifted higher for high $r_1$ and $r_2$ 

```{r}
ggplot(out) +
  geom_point(aes(x = r2, y = estimate)) +
  geom_errorbar(aes(x = r2, ymin = lower, ymax = upper)) +
  theme_gray(16) +
  xlab("Prior standard deviation of each group mean\n(r1 and r2)") +
  ylab("Fold change")
```

## Convergence diagnostics

MCMC uses Markov chains to produce posterior samples, and these Markov chains usually need thousands of iterations to converge to the posterior distribution. Convergence is essential. Otherwise, the samples do not faithfully reflect the posterior distribution. Unfortunately, it is impossible to prove convergence with 100% certainty. However, there are several diagnostics to check for obvious problems with convergence.

One such diagnostic is effective sample size (ESS), which incorporates the autocorrelation of the MCMC to quantify how exhaustively each Markov chain ranges over the parameter space. Ideally, the MCMC samples should have zero (or negative) autocorrelation, which frees them to explore all regions of the posterior. However, sometimes Markov chains get stuck in uncooperative regions, or they have trouble moving over the parameter space. In these difficult cases, we say there are fewer *effective* samples than actual samples because a positively autocorrelated sample is worth less than an independent sample. The number of effective samples is like the number of saved iterations, but ajusted based on the observed autocorrelation. There are different versions of effective sample size: for example, bulk effective sample size reflects the main section of the posterior, and tail effective sample size reflets the outer regions of the distribution. At a minimum, each parameter should have an effective sample size (ESS) of at least 100 times the number of Markov chains.

Another diagnostic is the Gelman-Rubin potential scale reduction factor, or $\widehat{R}$. For each scalar parameter, $\widehat{R}$ compares the variance among Markov chains to the variance of the parameter within each chain. If $\widehat{R}$ equals 1 for all parameters, this is evidence of "mixing": i.e., that the Markov chains approximate the same probability distribution. If the Markov chains started from different and dispersed starting values, then $\widehat{R}$ values of 1 lend evidence to convergence. An $\widehat{R}$ value above 1.01 is cause for concern. Something may be wrong with the model or data, or you may need to run the model for more iterations in any or all the phases of the Gibbs sampler (adaptation, warmup, and saved iterations). For our model, all the $\widehat{R}$ values are acceptable. $\widehat{R}$ may be a less sensitive diagnostic than ESS, but because it compares multiple chains against one another, it is more likely to detect if e.g. one of the chains is stuck in an uncooperative posterior mode the entire time.

Both ESS and $\widehat{R}$ are available by default in the `summarize_draws()` function of the posterior package. The results are acceptable for our qPCR model.

```{r, message = FALSE}
library(posterior)
samples %>%
  summarize_draws() %>%
  select(ess_bulk, ess_tail, rhat)
```

Traceplots are a graphical version of the same concept as $\widehat{R}$. A traceplot visualizes each MCMC chain against the iteration index, and it shows how the chains behave over time. In a well-behaved traceplot like the one below, the chains are heavily overplotted, and the chains do not trend up or down over time. Random fluctuation around the overall trend is a good sign that the chains are fully exploring the support of the posterior.

```{r, message = FALSE}
library(bayesplot)
mcmc_trace(as_draws(samples), pars = c("mu[1]", "mu[2]", "sigma"))
```

Likewise, it is useful to see the marginal posterior distribution of each parameter, as well as correlations among different scalar parameters. Algorithms like Gibbs sampling perform best when parameters are uncorrelated in the MCMC.

```{r}
mcmc_pairs(as_draws(samples), pars = c("mu[1]", "mu[2]", "sigma"))
```

## `brms`

We fit the model in `brms`, obtain posterior samples of the parameters, and check convergence diagnostics.

```{r, output = FALSE, warning = FALSE, eval = FALSE}
#Priors
prior <- get_prior(y ~ 0 + i, data = mutate(data, i = ordered(i)),
                    family = gaussian())
## define a prior on all population-level effects at once
prior$prior[1] <- "normal(0,10)"
## define a prior on the residual sd
prior$prior[4] <- "uniform(0.001,10)"
## verify that the priors indeed found their way into Stan's model code
make_stancode(y ~ 0 + i, data = mutate(data, i = ordered(i)),
                    family = gaussian(), 
                    prior = prior)
#Fit the model
fit_bayesian0 <- brm(y ~ 0 + i, data = mutate(data, i = ordered(i)),
                    family = gaussian(), prior = prior,
                    warmup=2e3,iter=4e3,chains=4,thin=1 
                    )
#Update function:
fit_bayesian <- update(fit_bayesian0,warmup=4e3,iter=8e3,chains=4,thin=2)
fit_bayesian %>% tidy() %>% 
                 rename(standard_error = std.error, lcl=conf.low, ucl=conf.high) %>%
                 select(term, estimate, standard_error,lcl,ucl)
#Convergence diagnostics
launch_shinystan(fit_bayesian)
#Posterior samples 
ps <- as_draws_df(fit_bayesian)
```

We check convergence. Rhat should be below 1.01, and effective sample size should be at least 100 times the number of chains (in our case, 400).

Computing the mean and 95% posterior interval of the fold change $\mu_2/\mu_1$ is easy in the Bayesian paradigm. We simply take the ratio of the posterior samples of $\mu_2$ and $\mu_1$. Unlike the delta method and Fieller's method, this method in the Bayesian paradigm is exact.

```{r, eval = FALSE}
bayesian_estimate <- as_draws_df(fit_bayesian) %>% 
                     summarise(estimate=mean(b_i1/b_i2),
                     lower = quantile((b_i1/b_i2), prob = 0.025),
                     upper = quantile((b_i1/b_i2), prob = 0.975) )
bayesian_estimate
```

Now, we can compare intervals.

```{r, eval = FALSE}
estimates <- bind_rows(
  delta = delta_approximation,
  fieller = fieller_approximation,
  bayes = bayesian_estimate,
  .id = "method"
)
estimates
```

```{r, eval = FALSE}
library(ggplot2)
ggplot(estimates) +
  geom_point(aes(x = method, y = estimate, color = method)) +
  geom_errorbar(aes(x = method, ymin = lower, ymax = upper, color = method)) +
  expand_limits(y = 0)
```

## Remarks

The fold change on the log scale is not the only possible measure of differential expression in animal studies, but we include it here because convenient frequentist approximations exist and match the results of an equivalent Bayesian analysis with diffuse priors. For more complicated estimands, frequentist approximations may not be feasible, but the Bayesian paradigm still just as easily yields a valid marginal posterior distribution.

## References