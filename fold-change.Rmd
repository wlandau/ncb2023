---
title: "Fold change example"
author: "Will Landau"
output: html_document
bibliography: bibliography.bib
---

```{r setup, include = FALSE}
library(coda)
library(rjags)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
set.seed(0)
```

## About

Consider a pre-clinical animal study (mice) with one treatment group and one placebo group. The outcome measure is qPCR gene expression on the log scale, and the goal is to estimate fold change of the outcome for treatment versus placebo. This report compares a simple frequentist model and the analogous Bayesian model in this analysis. In each case, the fold change is not an explicit parameter in the model and needs to be calculated by transforming other model parameters.

## Data

We simulate 10 mice from a normal distribution for each of the two groups.

Quantity | placebo | Treatment
---|---|---
Mean | 3 | 4
Standard deviation | 1 | 1

In our dataset, the `y` column is the outcome we are modeling, and the `i` column is the group designation.

```{r}
library(tidyverse)
placebo <- tibble(
  y = rnorm(n = 10, mean = 3, sd = 1),
  i = 1
)
treatment <- tibble(
  y = rnorm(n = 10, mean = 4, sd = 1),
  i = 2
)
data <- bind_rows(placebo, treatment)
data
```

## Frequentist analysis

The frequentist model is a simple cell means model with one mean for each group. Below, $i = 1, I$ ($I = 2$) is the group designation, $j = 1, \ldots, J$ ($J = 10$) is the index of the mouse, $y_{ij}$ is the response of group $i$ mouse $j$, $\mu_i$ is the unknown mean parameter of group $i$, $\sigma$ is the unknown residual standard deviation, and $\stackrel{\text{ind}}{\sim}$ is shorthand to indicate independent and identically distributed quantities.

$$
\begin{aligned}
y_{ij} \stackrel{\text{ind}}{\sim} \text{Normal}(\mu_i, \sigma^2)
\end{aligned}
$$
The fold change we want to estimate is the ratio of group means:

$$
\text{Fold change} = \frac{\mu_2}{\mu_1}
$$

The joint density of $y = (y_{11}, \ldots, y_{IJ})$ given the parameters can be written as

$$
p_{\mu_1, \mu_2, \sigma}(y) = \prod_{j = 1}^{J} \frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{1}{2} \left (\frac{y_{ij} - \mu_i}{\sigma} \right )^2}
$$

with likelihood $L(\mu_1, \mu_2, \sigma | y) = p_{\mu_1, \mu_2, \sigma}(y)$. The fold change we want to estimate is 

We fit this model using `lm()` in R and note the estimate and standard error of each mean (placebo and treatment).

```{r}
fit_frequentist <- lm(y ~ 0 + i, data = mutate(data, i = ordered(i))) %>%
  broom::tidy() %>%
  rename(standard_error = std.error) %>%
  select(term, estimate, standard_error)

fit_frequentist
```

We can use the delta method ([@beyene2005]) to approximate the mean and 95% confidence interval of the fold change $\mu_2/\mu_1$. The implementation below assumes independence between $\mu_1$ and $\mu_2$, which is consistent with the model.

```{r}
delta_method <- function(
  estimate_treatment,
  estimate_placebo,
  standard_error_treatment,
  standard_error_placebo,
  alpha = 0.05
) {
  a <- estimate_treatment
  b <- estimate_placebo
  se_a <- standard_error_treatment
  se_b <- standard_error_placebo
  theta <- a / b
  v11 <- se_a ^ 2
  v22 <- se_b ^ 2
  z <- qnorm(p = alpha / 2, lower.tail = FALSE)
  sigma <- sqrt((1 / (b ^ 2)) * (v11 + (theta ^ 2) * v22))
  lower <- theta - z * sigma
  upper <- theta + z * sigma
  tibble(estimate = theta, lower = lower, upper = upper)
}

index_treatment <- which(fit_frequentist$term == "i2")
index_placebo <- which(fit_frequentist$term == "i1")

delta_approximation <- delta_method(
  estimate_treatment = fit_frequentist$estimate[index_treatment],
  estimate_placebo = fit_frequentist$estimate[index_placebo],
  standard_error_treatment = fit_frequentist$standard_error[index_treatment],
  standard_error_placebo = fit_frequentist$standard_error[index_placebo],
  alpha = 0.05
)

delta_approximation
```

We can alternatively use Fieller's method ([@fieller1940], [@cordell1999], [@beyene2005]) to approximate the same quantities. The implementation below assumes independence between $\mu_1$ and $\mu_2$, which is consistent with the model.

```{r}
fieller_method <- function(
  estimate_treatment,
  estimate_placebo,
  standard_error_treatment,
  standard_error_placebo,
  alpha = 0.05
) {
  a <- estimate_treatment
  b <- estimate_placebo
  se_a <- standard_error_treatment
  se_b <- standard_error_placebo
  theta <- a / b
  v11 <- se_a ^ 2
  v22 <- se_b ^ 2
  z <- qnorm(p = alpha / 2, lower.tail = FALSE)
  k <- (z ^ 2 * v22) / b ^ 2
  estimate <- theta + (k / (1 - k)) * theta
  margin <- z / (b * (1 - k)) * sqrt(v11 + theta ^ 2 * v22 - k * v11)
  bound1 <- estimate - margin
  bound2 <- estimate + margin
  lower <- pmin(bound1, bound2)
  upper <- pmax(bound1, bound2)
  tibble(estimate = estimate, lower = lower, upper = upper)
}

fieller_approximation <- fieller_method(
  estimate_treatment = fit_frequentist$estimate[index_treatment],
  estimate_placebo = fit_frequentist$estimate[index_placebo],
  standard_error_treatment = fit_frequentist$standard_error[index_treatment],
  standard_error_placebo = fit_frequentist$standard_error[index_placebo],
  alpha = 0.05
)

fieller_approximation
```

## Bayesian model

The Bayesian model is the same as the frequentist one except that we treat parameters as if they were random variables and we assign them prior distributions.

$$
\begin{aligned}
&y_{ij} \stackrel{\text{ind}}{\sim} \text{Normal}(\mu_i, \sigma^2) \\
&\qquad \mu_i \stackrel{\text{ind}}{\sim} \text{Normal}(m_i, r_i^2) \\
&\qquad \sigma \sim \text{Uniform}(0, s)
\end{aligned}
$$

Above, the $m_i$'s, the $r_i$'s, and $s$ are constant hyperparameters that placebo the diffuseness of the priors. If we lack prior information, diffuse priors (e.g. large values of $r_1$, $r_2$, and $s$) may be appropriate if this is computationally feasible.

We express the model in the JAGS code below.^[The normal distribution in JAGS accepts the precision as the second argument instead of the variance or standard deviation.]

```{r}
code <- "
model{
  for (n in 1:N) {
    y[n] ~ dnorm(mu[i[n]], (1 / (sigma * sigma)))
  }
  for (i in 1:I) {
    mu[i] ~ dnorm(m[i], (1 / (r[i] * r[i])))
  }
  sigma ~ dunif(0, s)
}
"
```

We save this code to a file.

```{r, include = FALSE}
jags_file <- tempfile()
writeLines(code, jags_file)
```

We fit the model in `rjags`, obtain posterior samples of the parameters, and check convergence diagnostics.

```{r, output = FALSE, warning = FALSE}
library(rjags)
jags_data <- list(
  y = data$y,
  i = data$i,
  N = nrow(data),
  I = length(unique(data$i)),
  m = c(0, 0),
  r = c(10, 10),
  s = 10
)
n_chains <- 4
n_adapt <- 2e3
n_warmup <- 2e3
n_iterations <- 4e3
inits <- replicate(
  n_chains,
  list(
    ".RNG.name" = "base::Mersenne-Twister",
    ".RNG.seed" = sample.int(n = 1e6, size = 1)
  ),
  simplify = FALSE
)
model <- rjags::jags.model(
  file = jags_file,
  data = jags_data,
  n.chains = n_chains,
  n.adapt = n_adapt,
  inits = inits
)
update(model, n.iter = n_warmup, quiet = quiet)
coda <- rjags::coda.samples(
  model = model,
  variable.names = c("mu", "sigma"),
  n.iter = n_iterations
)
samples <- tibble::as_tibble(posterior::as_draws_df(coda))
```

We check convergence. Rhat should be below 1.01, and effective sample size should be at least 100 times the number of chains (in our case, 400).

```{r}
samples %>%
  posterior::summarise_draws() %>%
  summarize(
    max_rhat = max(rhat),
    min_ess_bulk = min(ess_bulk),
    min_ess_tail = min(ess_tail)
  )
```

Computing the mean and 95% posterior interval of the fold change $\mu_2/\mu_1$ is easy in the Bayesian paradigm. We simply take the ratio of the posterior samples of $\mu_2$ and $\mu_1$. Unlike the delta method and Fieller's method, this method in the Bayesian paradigm is exact.

```{r}
samples_fold_change <- samples$`mu[2]` / samples$`mu[1]`
bayesian_estimate <- tibble(
  estimate = mean(samples_fold_change),
  lower = quantile(samples_fold_change, prob = 0.025),
  upper = quantile(samples_fold_change, prob = 0.975)
)

bayesian_estimate
```

Now, we can compare intervals.

```{r}
estimates <- bind_rows(
  delta = delta_approximation,
  fieller = fieller_approximation,
  bayes = bayesian_estimate,
  .id = "method"
)
estimates
```

```{r}
library(ggplot2)
ggplot(estimates) +
  geom_point(aes(x = method, y = estimate, color = method)) +
  geom_errorbar(aes(x = method, ymin = lower, ymax = upper, color = method)) +
  expand_limits(y = 0)
```

## Priors

So far, we have been using diffuse priors for $\mu_1$ and $\mu_2$. The goal for the priors was to be "non-informative", i.e. to reduce the influence of the prior distribution as much as possible and let the data drive the results. But in some situations, e.g. for computational reasons or to make us of historical information, it is desirable to use more informative priors. In cases like these, it is useful to run sensitivity analyses to understand the effect of the prior on the marginal posterior of the quantity of interest. Below, we experiment with a grid of $r_1$ and $r_2$ values and show the resulting posterior interval of $\mu_2/\mu_1$ for each analysis. For this case, suppose we choose prior means $m_1 = m_2 = 3$ based on historical data or expert elicitation. Low $r_i$ values impose a tight induced prior for $\mu_2/\mu_1$ centered at 1 (lower variance at the cost of increase bias towards a lack of differential expression).

```{r}
single_analysis <- function(
  data,
  jags_file,
  m1 = 0,
  m2 = 0,
  r1 = 10,
  r2 = 10
) {
  jags_data <- list(
    y = data$y,
    i = data$i,
    N = nrow(data),
    I = length(unique(data$i)),
    m = c(m1, m2),
    r = c(r1, r2),
    s = 10
  )
  n_chains <- 4
  n_adapt <- 2e3
  n_warmup <- 2e3
  n_iterations <- 4e3
  inits <- replicate(
    n_chains,
    list(
      ".RNG.name" = "base::Mersenne-Twister",
      ".RNG.seed" = sample.int(n = 1e6, size = 1)
    ),
    simplify = FALSE
  )
  model <- rjags::jags.model(
    file = jags_file,
    data = jags_data,
    n.chains = n_chains,
    n.adapt = n_adapt,
    inits = inits
  )
  update(model, n.iter = n_warmup, quiet = quiet)
  coda <- rjags::coda.samples(
    model = model,
    variable.names = c("mu", "sigma"),
    n.iter = n_iterations
  )
  samples <- tibble::as_tibble(posterior::as_draws_df(coda))
  samples_fold_change <- samples$`mu[2]` / samples$`mu[1]`
  summary <- posterior::summarise_draws(samples)
  tibble::tibble(
    estimate = mean(samples_fold_change),
    lower = quantile(samples_fold_change, prob = 0.025),
    upper = quantile(samples_fold_change, prob = 0.975),
    m1 = m1,
    m2 = m2,
    r1 = r1,
    r2 = r2,
    max_rhat = max(summary$rhat),
    min_ess_bulk = min(summary$ess_bulk),
    min_ess_tail = min(summary$ess_tail)
  )
}
```

```{r, include = FALSE}
library(purrr)
out <- map_dfr(
  .x = seq_len(20) / 10,
  ~single_analysis(
    data = data,
    jags_file = jags_file,
    m1 = 3,
    m2 = 3,
    r1 = .x,
    r2 = .x
  )
)
```

Below, we plot the marginal posterior mean and 95% posterior interval of $\mu_2/mu_1$. Each interval corresponds to an independent Bayesian analysis with a different value of $r_1$. As expected, we see a narrow interval centered at 1 for small $r_1$ and $r_2$ and a wider interval shifted higher for high $r_1$ and $r_2$ 

```{r}
ggplot(out) +
  geom_point(aes(x = r2, y = estimate)) +
  geom_errorbar(aes(x = r2, ymin = lower, ymax = upper)) +
  theme_gray(16) +
  xlab("Prior standard deviation of each group mean\n(r1 and r2)") +
  ylab("Fold change")
```


## Remarks

The fold change on the log scale is not the only possible measure of differential expression in animal studies, but we include it here because convenient frequentist approximations exist and match the results of an equivalent Bayesian analysis with diffuse priors. For more complicated quantities of interest, frequentist approximations may not be feasible, but the Bayesian paradigm still just as easily yields a valid marginal posterior distribution.

## References