---
title: "Stability of Dissolution"
author: "Luwis Diya"
date: "`r format(Sys.time(), '%d %B %Y (%X)')`"
output: html_document 
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dev = "png",
  fig.ext = ".png"
)
options(max.print = 999999, warn = -1)
set.seed(05102008)
fs::dir_create("output")
```

```{r packages, echo = FALSE, eval = TRUE, message = FALSE, include = FALSE}
library(tidyverse)
library(plyr)
library(captioner)
library(ggplot2)
library(readxl)
library(DT)
library(kableExtra)
library(glue)
library(lme4)
library(lmerTest)
library(rstan)
library(brms)
library(R2jags)
library(shinythemes)
library(shinystan)
library(mcmcplots)
library(bayesplot)
library(ggmcmc)
library(superdiag)
library(tidybayes)
library(broom)
library(broom.mixed)
```

```{r options-and-functions, echo = TRUE, eval = TRUE, message = FALSE, include = FALSE}
rstan_options(auto_write = TRUE)
options(max.print = 2e6, mc.cores = parallel::detectCores())
lapply(list.files("R", full.names = TRUE), source)
Q <- 80
```

```{r tables, echo = TRUE, eval = TRUE, message = FALSE, include = FALSE}
tbls <- captioner(prefix = "Table")
tbls("Tab1","Stability data structure")
tbls("Tab2","Summary statistics of dissolution data by storage condition")
tbls("Tab3","Frequentist variable selection")
tbls("Tab4","Frequentist: model fit statistics")
tbls("Tab5","Frequentist: fixed effects")
tbls("Tab6","Frequentist: variance components")
tbls("Tab7","Frequentist: augmented data")
tbls("Tab8","Instantly switching from frequentist[lme4/lmer] to Bayesian[brm/brm]")
tbls("Tab9","Model summary based on the Bayesian approach")
tbls("Tab10","Summary of the final model")
tbls("Tab11","Variance Components")
tbls("Tab12","Fixed Effects")
tbls("Tab13","The Annual Rate of Change")
```

```{r figures, echo = TRUE, eval = TRUE, message = FALSE, include = FALSE}
figs <- captioner(prefix = "Figure")
figs(
  "Fig1",
  "Stability profile of dissolution values (% dissolved) by batch and storage condition"
)
figs("Fig2","Predicted stability profile of dissolution values (% dissolved) by batch and storage condition")
```

```{r data, echo=FALSE, eval=TRUE, message=FALSE, include=FALSE}
# Reading in the data.
dataset <- readRDS("data/simdata.rds") |>
  mutate(RunID = cumsum(!duplicated(cbind(Batch, Condition,Stabtime))))
# Plot the dataset [dissolution profiles].
zerodata2  <- dataset |> filter(Stabtime == 0)
zerodata1  <- zerodata2 |>
  mutate(Condition = levels(dataset$Condition)[1])
zerodata3  <- zerodata2 |>
  mutate(Condition = levels(dataset$Condition)[3])
zerodata4  <- zerodata2 |>
  mutate(Condition = levels(dataset$Condition)[4])
zerodata5  <- zerodata2 |>
  mutate(Condition = levels(dataset$Condition)[5])
plot_data  <- rbind(
  dataset,
  zerodata1,
  zerodata2,
  zerodata3,
  zerodata4,
  zerodata5
)

# Prediction dataset [for "25°C/60%RH" & "30°C/75%RH"].
tmpx <- dataset |>
  filter(
    (Condition==levels(Condition)[2]) | (Condition==levels(Condition)[3]),
    Stabtime > 2,
    Stabtime < 10
  ) |>
  mutate(
    Stabtime = ifelse(Stabtime == 3, 18, ifelse(Stabtime == 6, 24, 36)),
    Result = NA
  )
new_dataset <- rbind(plot_data, tmpx)
```

# Introduction

The objective of this study is a statistical evaluation to assess the risk of 
failing dissolution stage testing (Immediate-Release Dosage Forms) at 0, 
24 and 36 months for the Active Pharmaceutical Ingredient (API) A  
in batches of AB Fixed Dose Combination (FDC) Film Coated Tablets (FCT) 
packaged in blisters at dissolution time points 25 minutes and for storage 
conditions 25°C/60%RH and 30°C/75%RH. The "specification limit" or Q value is 
`r Q`\%.

```{r slides, echo = FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("figures/slide1.png")
knitr::include_graphics("figures/slide2.png")
knitr::include_graphics("figures/slide3.png")
knitr::include_graphics("figures/slide4.png")
```

# Data

The dataset consists of stage I (one dissolution bath i.e., 6 vessels) 
dissolution (% dissolved) data for the active A in 
`r length(unique(dataset$Batch))` batches (`r levels(dataset$Batch)`) of 
AB FDC FCT packaged in blisters. 

Data from 6 vessels were available per individual batch and stability time point. 
`r  tbls("Tab1", display = "cite")` shows the stability data structure. 

```{r datastr, echo=FALSE}
tbls("Tab1", display = "full")
dataset %>%
  distinct(Condition, Stabtime) |> 
  spread(Stabtime, Stabtime) %>% 
  mutate(St = gsub(',NA', '', gsub('NA,', '', paste(`0`, `3`, `6`, `9`, `12`, `18`, sep = ",")))) |> 
  dplyr::select(Condition, St) |> 
  spread(Condition, St) |> 
  dplyr::select(`5°C`, `25°C/60%RH`, `30°C/75%RH`, `40°C/75%RH`, `50°C`) |> 
  knitr::kable() |>  
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

A listing of the data is shown in Appendix.  

# Methods

A Bayesian linear mixed effects model accounting for the process mean at initial, 
condition specific rate of change, random components due to batch-to-batch and 
run-to-run (possibly including other uncontrolled sources of variability which 
impact the analytical run) variability  was fitted. The model is as follows:

$$ y_{l(ijk)} = \mu + \alpha_{i}+\gamma_{ijk}+\beta_{j}\times t_{ijk}+\epsilon_{l(ijk)}$$
where 

*   $y_{l(ijk)}$  = dissolution of the l-th vessel within the i-th study and j-th condition at the l-th stability time,
*   $\mu$ = is the process mean at initial,
*   $\alpha_{i}$ = random effect from the i-th study: $\alpha_{i} \sim N(0,\sigma_{\alpha}^{2})$,
*   $\gamma_{ijk}$ = analytical run variability: $\gamma_{ijk} \sim N(0,\sigma_{\gamma}^{2})$,
*   $\beta_{j}$ = fixed rate of change for the j-th condition,
*   $\epsilon_{l(ijk)}$ = vessel-to-vessel or residual variability: $\epsilon_{l(ijk)} \sim N(0,\sigma_{\epsilon}^{2})$. 

## Bayes theorem

$$ P(\theta|Data) = \frac{P(Data|\theta)\times P(\theta)}{P(Data)}$$
where 

*   $P(\theta|Data)$  = posterior probability of parameters ($\theta$) given data [updated belief]
*   $P(Data|\theta)$  = the likelihood [the experiment or study],
*   $P(\theta)$       = prior(s): past state of knowledge or beliefs [previous experiments/studies, expert opinion or your own belief] and
*   $P(Data)$         = marginal probability of the data
 
Note that in the Bayesian linear mixed effects model above, I have stated only 
the likelihood ($P(Data|\theta)$)  and nothing is mentioned about the prior(s) 
($P(\theta)$). Note also that the complete likelihood include the family of 
distributions (default: Gaussian/Normal) and link function (default: identity link).

## Dissolution acceptance criteria
The acceptance criteria of dissolution for Immediate-Release Dosage Forms are 
listed below:

*   Stage 1: Test 6 tablets (one dissolution bath). Accept the batch if each unit 
is not less than $Q+5\%$, otherwise go to stage 2.

*   Stage 2: Test 6 additional tables (second dissolution bath). Accept the batch 
if the average of 12 units (Stage 1 $+$ Stage 2) is equal to or greater than $Q$, 
and no unit is less than $Q-15\%$. If the batch fails Stage 2 then go to Stage 3.

*   Stage 3: Test 12 additional tables (third and fourth dissolution baths). Accept 
a batch if the average of 24 units (Stage 1 $+$ Stage 2 $+$ Stage 3) is equal to 
or greater than $Q$, not more than 2 units are less than $Q-15\%$ and no unit is 
less than $Q-25\%$. If the batch fails this stage then it is rejected.

Note that, $Q$ is the amount of dissolved active ingredient specified in the 
individual monograph, expressed as a percentage of the labeled content.

# Results{.tabset}
## Exploratory data analysis{.tabset}
### Summary statistis

`r tbls("Tab2", display = "cite")` shows the descriptive statistics of  
dissolution data by storage condition. Note that the descriptive statistics are 
collapsed over stability protocol, Zytiga granulate manufacturing 
site, Zytiga synthesis route and stability time.

```{r summ, echo=FALSE}
tbls("Tab2", display = "full") 

## Summary (for in-text recall):
SummTable <- dataset |>   
  group_by(Condition) |>  
  dplyr::summarize(
    Observations = n(), 
    Mean = round_any(mean(Result), 0.1, round),
    SD = round_any(sd(Result),0.01,round),                 
    "Mean (SD)" = glue("{format(Mean,nsmall=1)}({format(SD,nsmall=1)})"),
    Minimum = round_any(min(Result), 0.1, round),
    Maximum = round_any(max(Result), 0.1, round)
  ) 

## Summary (print table):
SummTable %>% mutate(
  Minimum = format(Minimum,nsmall=1), 
  Maximum = format(Maximum,nsmall=1)
) |> 
  dplyr::select(-c(Mean,SD)) %>%
  knitr::kable() |> 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

### Exploratory plots
`r figs("Fig1", display = "cite")` shows the stability profile of dissolution 
data by batch and storage condition. The blue dashed line represent $Q+5$ value 
(85% dissolved) and the  dashed red line represent the $Q$ value (80% dissolved). 


```{r plt1, echo=FALSE}
ggplot(plot_data, aes(x = Stabtime, y = Result,  group=Batch,col = Batch)) + 
  geom_jitter(width = 0.2,alpha = 0.1) + 
  facet_grid(.~Condition)+  
  stat_summary(fun = "mean", geom = "line") + 
  stat_summary(fun = "mean", geom = "point",shape="X",size = 2) + 
  xlab("Stability time (months)") + 
  ylab("Dissolution value (% dissolved)") +
  geom_hline(yintercept = 85, col ="blue",linetype = "dashed") + 
  geom_hline(yintercept = 80, col = "red", linetype = "dashed" ) +   
  theme(
    legend.position="none",
    axis.ticks = element_blank(),
    axis.text.x = element_text(angle = 0, hjust = 1,vjust = 0.5)
  )
figs("Fig1", display = "full")
```

## Frequentist approach{.tabset}

The frequentist approach restrict our view to the current experiment or studies 
i.e., $P(Data|\theta)$. It focuses on only one part of the Bayes theorem and 
claim "ignorance" of the past state of knowledge (i.e., the prior(s) $P(\theta)$) 
on the current process or related processes.


```{r FanalysisVS, echo = TRUE}
## Final model
fmod <- lmer(Result ~ Stabtime:Condition + (1 | RunID) + (1 | Batch), data = dataset)
```

### Glance
`r tbls("Tab4", display = "cite")` shows a glance of the model fit summaries from  
the frequentist approach.

```{r FanalysisG, echo = TRUE}
tbls("Tab4", display = "full") 
glance(fmod) %>%
  knitr::kable() |> 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

### Fixed effects

`r tbls("Tab5", display = "cite")` shows the fixed effects model summaries from
the frequentist approach.

```{r FanalysisTSF, echo = TRUE}
tbls("Tab5", display = "full") 
tidy(fmod, effects="fixed", conf.int = TRUE, conf.level = 0.95)[, -1] |> 
mutate(
  Mean = round_any(estimate, 0.01),
  StErr = round_any(std.error, 0.001),
  P.value = ifelse(
    round_any(p.value, 0.01) < 0.01,
    round_any(0.01, 0.01),
    round_any(p.value, 0.01)
  ),
  LCL = round_any(conf.low, 0.01), UCL = round_any(conf.high, 0.01),
  "Mean (SD)" = glue("{format(Mean,nsmall=2)}({format(StErr,nsmall=3)})"),
  Pvalue = format(P.value,nsmall = 2),
  Term = term,
  "95% CI" = glue("({format(LCL,nsmall=2)};{format(UCL,nsmall=2)})")) |> 
  dplyr::select(Term, `Mean (SD)`, `95% CI`, Pvalue) |>  
  knitr::kable() |> 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

### Variance components

`r tbls("Tab6", display = "cite")` shows the variance components summaries from 
the frequentist approach.

```{r FanalysisTSV, echo = TRUE}
tbls("Tab6", display = "full") 
tidy(fmod,effects = "ran_pars")[, -c(1,3)] %>% 
  mutate(
    VarianceComponent = group,
    Mean_ = round_any(estimate, 0.01),
    Mean = format(Mean_, nsmall=2)
  ) %>% 
  dplyr::select(VarianceComponent, Mean) %>% 
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

### Augmented data
`r tbls("Tab7", display = "cite")` shows the augmented data from the frequentist 
approach.

```{r FanalysisTSA, echo = TRUE}
tbls("Tab7", display = "full") 
head(
  augment(fmod)[, c(1:9,16)]) %>%
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

## Bayesian approach: JAGS {.tabset}

### Bayes theorem
The Bayesian approach utilize all the components mentioned in Bayes theorem i.e., 
taking the current experiment and the prior beliefs (totality of information).

$$ P(\theta|Data) = \frac{P(Data|\theta)\times P(\theta)}{P(Data)}$$
### Priors
The priors are listed below:

.# Priors:

  .## fixed effects

    for(f in 1:6){beta[f]~ dnorm(0.0,1.0E-3)}

  .## Precision priors

  taue ~ dgamma(1.0E-3,1.0E-3)

  taur ~ dgamma(1.0E-3,1.0E-3)

  taub ~ dgamma(1.0E-3,1.0E-3)

  .## variance components

  sigmae2 <-1.0/(taue)

  sigmar2 <-1.0/(taur)

  sigmab2 <-1.0/(taub)"

### Likelihood

  .# Likelihood

	for(k in 1:N){

	y[k] ~ dnorm(mu[k], taue)

        mu[k]<-beta[1]+(beta[2]*c1[k]+beta[3]*c2[k]+beta[4]*c3[k]+beta[5]*c4[k]+
               beta[6]*c5[k])*t[k]+b[bid[k]]+r[rid[k]]
	      
        }

        for (i in 1:B) {b[i] ~ dnorm(0,taub)}

        for (j in 1:R) {r[j] ~ dnorm(0,taur)}

### JAGS code
By putting together the likelihood and the prior we get the posterior.

model {

  .# Likelihood

	for(k in 1:N){

	y[k] ~ dnorm(mu[k], taue)

        mu[k]<-beta[1]+(beta[2]*c1[k]+beta[3]*c2[k]+beta[4]*c3[k]+beta[5]*c4[k]+
               beta[6]*c5[k])*t[k]+b[bid[k]]+r[rid[k]]
	      
        }

        for (i in 1:B) {b[i] ~ dnorm(0,taub)}

        for (j in 1:R) {r[j] ~ dnorm(0,taur)}

  .# Priors:

  .## fixed effects

    for(f in 1:6){beta[f]~ dnorm(0.0,1.0E-3)}

  .## Precision priors

  taue ~ dgamma(1.0E-3,1.0E-3)

  taur ~ dgamma(1.0E-3,1.0E-3)

  taub ~ dgamma(1.0E-3,1.0E-3)

  .## variance components

  sigmae2 <-1.0/(taue)

  sigmar2 <-1.0/(taur)

  sigmab2 <-1.0/(taub)
}

### Run model
```{r JAGSbayesian, echo = TRUE}
#----------------------------- Global parameters ------------------------------#
nsims <- 2000
nburn <- 1000
nthin <- 1
#------------------------ Tolerance interval parameters -----------------------#
Betat <- 0.95 # content
Gammat <- 0.95 # confidence

#------------------------- Specifications (Q value)  --------------------------#
Q <- 80
#------------------------------ Analysis subset -------------------------------#
N <- dim(dataset)[1]
B <- max(as.numeric(dataset$Batch))
R <- max(as.numeric(dataset$RunID))
bid <- dataset$Batch
rid <- dataset$RunID
y <- dataset$Result
t <- dataset$Stabtime
c1 <- ifelse(dataset$Condition == unique(dataset$Condition)[1], 1, 0)
c2 <- ifelse(dataset$Condition == unique(dataset$Condition)[2], 1, 0)
c3 <- ifelse(dataset$Condition == unique(dataset$Condition)[3], 1, 0)
c4 <- ifelse(dataset$Condition == unique(dataset$Condition)[4], 1, 0)
c5 <- ifelse(dataset$Condition == unique(dataset$Condition)[5], 1, 0)

## for simulation:
B2 <- 20
N2 <- R2 <- 400
rid2 <- 1:R2
bid2 <- rep(1:20, each = 20)
#-------------------------------- jags subset ---------------------------------#
dataseta <- c(
  "N", "N2", "B", "B2", "R", "R2", "y", "t",
  "c1", "c2", "c3", "c4", "c5", "bid", "bid2",
  "rid", "rid2", "Q"
)

#--------------------------------- Parameters ---------------------------------#
#All parameters of interest (modeled and derived)
param<-c(
  "beta","sigmae2","sigmar2","sigmab2",
  "poos0","poos25C24","poos25C36","poos30C24","poos30C36"
)

#Only modeled parameters
param0 <- c("beta", "sigmae2", "sigmar2", "sigmab2")

#------------------------------ Initial values --------------------------------#
inits <- list(
  list(
    beta = c(89, 0.04, 0.02, 0.04, 0.14, -0.07),
    taue = 1,
    taub = 1.25,
    taur = 1, 
    b = rnorm(B, 0, 1),
    r = rnorm(R, 0, 1)
  ),
  list(
    beta = c(93, 0.08, 0.01, 0.03, 0.12, -0.08),
    taue = 1,
    taub = 1.48,
    taur = 1, 
    b = rnorm(B, 0, 1),
    r = rnorm(R, 0, 1)
  ),
  list(
    beta = c(95, 0.06, 0.03, 0.02, 0.16, -0.09),
    taue = 1,
    taub = 1.30,
    taur = 1, 
    b = rnorm(B, 0, 1),
    r = rnorm(R, 0, 1)
  )
)

#---------------------------------- Run jags ----------------------------------#
# Start the clock!
ptm <- proc.time()
bmodj<- jags(
  data = dataseta,
  inits = inits,
  n.chains = 3,
  param,
  n.burnin = nburn, 
  n.iter = nsims,
  n.thin = nthin,
  model.file = "model.jags", 
  DIC = TRUE
)
bmodj0 <- jags(
  data = dataseta,
  inits = inits,
  n.chains = 3,
  param0,
  n.burnin = nburn, 
  n.iter = nsims,
  n.thin = nthin,
  model.file = "model.jags",
  DIC = TRUE
)
# Stop the clock
proc.time() - ptm
```


### Convergence diagnostics

```{r JAGSbayesianDiag, echo = TRUE}
#--------------------------- Convergence diagnostics --------------------------#
plot(bmodj0)
S <- as.mcmc(bmodj0)
plot(S)
crosscorr.plot(S)
#gelman.diag(S)
#gelman.plot(S)
```


### Model summaries

```{r JAGSbayesianMS, echo = TRUE}
#------------------------------ Posterior summary -----------------------------#
print(bmodj)
```

## Bayesian approach: brms {.tabset}
### brms R package: Bayesian Regression Models using Stan
The Bayesian approach has a steep learning curve. That is, not only does one 
have to deal with the likelihood specification but one has also to specify the 
priors when using software such as BUGS, JAGS, Nimble and Stan.

The R package brms try to flatten the "steep learning curve" by allowing an easy 
transition from the frequentist approach. The package brms mimics the algorithm 
of the widely used frequentist R package lme4. Let us fit our first Bayesian model.


`r tbls("Tab8", display = "cite")` shows how to instantly switch from the frequentist 
approach to the Bayesian approach. You just need to change one thing: lmer -> brm. 

```{r FBanalysis, echo = TRUE}
tbls("Tab8", display = "full")
# ## lme4: frequentist approach
# fmod <- lmer(Result~Stabtime:Condition+(1|RunID)+(1|Batch),data=dataset)
# ## brms: Bayesian approach
bmod0 <-  brm(
  Result ~ Stabtime:Condition + (1 | RunID) + (1 | Batch),
  data = dataset
)
# # save brms object:
# saveRDS(bmod0, "output/bmod0.rds")
# read brms object:
# bmod0 <- readRDS("output/bmod0.rds")
# summary(bmod0)
tidy(bmod0) %>%
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

### Priors

The code below allows for one to obtain the default priors for a given 
likelihood. If you don't specify the priors brms will use default priors. In 
case you are comfortable with the default priors but need to report the priors 
in your statistical report or manuscript you can also use the code.

```{r GetDefaultPriors, echo = TRUE}
# Get the default priors:
get_prior(
  Result ~ Stabtime:Condition + (1 | RunID) + (1 | Batch),
  data = dataset,
  family = gaussian()#,
  #link = ?
)
```

You can also specify new priors by starting with the default priors but altering 
some of the prior statements.

```{r YourPriors, echo = TRUE}
# Specify your own priors:
bprior <- c(
  prior_string("normal(0,10)", class = "b"),
  prior(normal(0, 5), class = b, coef = "Stabtime:Condition5°C"),
  prior(normal(90, 10), class = "Intercept"),
  prior_(~cauchy(0, 2), class = ~sd, group = ~RunID)
)
```

### Likelihood

The likelihood is given below:

*   $$ y_{l(ijk)} = \mu + \alpha_{i}+\gamma_{ijk}+\beta_{j}\times t_{ijk}+\epsilon_{l(ijk)}$$
*   family = Gaussian
*   link = identity link

### Stan code
```{r MakeStancode, echo = TRUE}
# Stan code:
make_stancode(
  Result ~ Stabtime:Condition + (1 | RunID) + (1 | Batch),
  data=dataset,
  family = gaussian()
)
```

### Run model

Combining the likelihood and the priors leads to the posterior distribution of 
the parameters of interest. The summaries of these parameters are then derived 
from the posterior distribution.

`r tbls("Tab9", display = "cite")` shows model summary based on the Bayesian approach. 

```{r Banalysis, echo = TRUE}
tbls("Tab9", display = "full")
# Run model:
#--- Comment out [Shift+Ctrl+C] to avoid a re-run after saving the object
bmod <-  brm(
  Result ~ Stabtime:Condition + (1 | RunID) + (1 | Batch),
  data = dataset,
  family = gaussian(),
  chains = 3,
  iter = 5000,
  warmup = 1000,
  thin = 1,
  prior = bprior,
  save_pars = save_pars(group = TRUE)
)
#----
# save brms object:
# saveRDS(bmod, "Output/bmod.rds")
# read brms object:
#bmod <- readRDS("Output/bmod.rds")
# Print model summaries
tidy(bmod) %>%
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

### Convergence diagnostics

Convergence diagnostics are essential prior to utilizing the results of a 
Bayesian analysis. There are many convergence diagnostics but in this study we 
will limit to just a few.

#### Names of parameters

```{r Parnames, echo = TRUE}
# names of parameters
#head(parnames(bmod),n=21) #deprecated
head(variables(bmod), n=21)
```

#### Plot function

```{r CDPlot, echo = TRUE}
#Trace and Density Plots for MCMC Draws
## All parameters:
plot(bmod, ask=FALSE)
## fixed effects:
#plot(bmod, variable = "b_Intercept")
plot(bmod, variable = "b_Stabtime:Condition25°CD60%RH")
## all fixed effects (regular expression):
plot(bmod, variable = "^b_", regex = TRUE)
## variance components:
plot(bmod, variable = "sigma")
# plot(bmod, variable = "sd_RunID__Intercept")
# plot(bmod, variable = "sd_Batch__Intercept")
## variance components excluding residual variance:
# plot(bmod, variable = "^sd_", regex = TRUE)
## all variance components:
plot(bmod, variable = "^s", regex = TRUE)
```

#### Pairs function

```{r PairsPlot, echo = TRUE}
pairs(bmod, variable = variables(bmod)[1:3])
# pairs(bmod, variable = "^b_", regex = TRUE)
# pairs(bmod, variable = "^sd_", regex = TRUE)
pairs(bmod, variable = "^s", regex = TRUE)
```


#### MCMCPlots package


```{r MCMCPlots, echo  =TRUE}
#Hist, Density, Trace plots
mcmc_plot(bmod, type = "hist") 
#mcmc_plot(bmod,type="dens_overlay")
mcmc_plot(bmod, type = "trace")
#Acf, Rhat, Neff 
mcmc_plot(bmod, type = "acf")#"acf_bar"
mcmc_plot(bmod, type = "rhat")
#mcmc_plot(bmod,type="neff")
```

#### CODA package

```{r CODA, echo = TRUE}
#Trace and Density Plots for MCMC Draws
bmod.mcmc <- as.mcmc(bmod)
gelman.diag(bmod.mcmc[, 1:10])
gelman.plot(bmod.mcmc[, 1:10])
# geweke.diag(bmod.mcmc[, 1:10])
# geweke.plot(bmod.mcmc[, 1:10])
# autocorr(bmod.mcmc[, 1:10])
# autocorr.diag(bmod.mcmc[, 1:10])
# autocorr.plot(bmod.mcmc[, 1:10])
# crosscorr(bmod.mcmc[, 1:10])
crosscorr.plot(bmod.mcmc[, 1:10])
#coda.menu()
```

#### Shiny Stan

```{r ShinyStan, echo = TRUE}
# shiny app:
#launch_shinystan(bmod)# not running
```

### Model update

```{r ModUpdate0, echo = TRUE}
#Update model model:
bmod2 <-  update(
  bmod,
  formula. = ~ . - (1|RunID),
  #newdata  = dataset,
  # family=student(),
  # prior=bprior2, # prior= set_prior("normal(0,5)"),
  chains = 3,
  iter = 20000,
  warmup = 10000,
  thin = 10
)
#save brms object:
saveRDS(bmod2, "output/bmod2.rds")
#read brms object:
bmod2 <- readRDS("output/bmod2.rds")
```

### Model comparison

```{r ModCompIC, echo = TRUE}
# WAIC:
bmod.waic  <- waic(bmod)
bmod2.waic <- waic(bmod2)
# compare both models
compare_ic(bmod.waic, bmod2.waic)
# # LOO:
# bmod.loo  <- waic(bmod)
# bmod2.loo <- waic(bmod2)
# # compare both models
# compare_ic(bmod.loo, bmod2.loo)
```

```{r ModCompLOO, echo = TRUE}
#loo_compare(x, ..., criterion = c("loo", "waic", "kfold"), model_names = NULL)
# WAIC:
## add waic to the models
bmod.add.waic  <- add_criterion(bmod, "waic")
bmod2.add.waic <- add_criterion(bmod2, "waic")
## compare both models
loo_compare(bmod.add.waic, bmod2.add.waic, criterion = "waic")
# # LOO:
# ## add loo to the models
# bmod.add.loo  <- add_criterion(bmod, "loo")
# bmod2.add.loo <- add_criterion(bmod2, "loo")
# ## compare both models
# loo_compare(bmod.add.loo, bmod2.add.loo, criterion = "loo")
# # KFOLD:
# ## add kfold to the models
# bmod.add.kfold  <- add_criterion(bmod, "kfold")
# bmod2.add.kfold <- add_criterion(bmod2, "kfold")
# ## compare both models
# loo_compare(bmod.add.kfold, bmod2.add.kfold, criterion = "kfold")
```

### Model summaries

`r tbls("Tab10", display = "cite")` shows the summary of the final model.

```{r ModSummaries, echo = TRUE}
tbls("Tab10", display = "full")
#Run model:
tidy(bmod) %>%
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

`r tbls("Tab11", display = "cite")` shows the summary of variance components.
```{r TidyModSummariesVC, echo = TRUE}
tbls("Tab11", display = "full")
tidy(
  bmod,
  parameters = NA, #parameters = "^s_"
  effects = "ran_pars",
  robust = TRUE, # Default Option: FALSE [mean]
  conf.level = 0.95,
  conf.method = "quantile" #c("quantile", "HPDinterval")
) |>  
  mutate(
    VarianceComponent = group,
    Median = round_any(estimate, 0.01), 
    MAD = round_any(std.error, 0.001),
    LCL = round_any(conf.low, 0.1),
    UCL = round_any(conf.high, 0.1),
    "Median (MAD)" = glue("{format(Median,nsmall=2)} ({format(MAD,nsmall=3)})"),
    "95% CI" = glue("({format(LCL,nsmall=2)} ;{format(UCL,nsmall=2)})")
  ) |>
  dplyr::select(VarianceComponent, `Median (MAD)`, `95% CI`) |> 
  knitr::kable() |> 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```


`r tbls("Tab12", display = "cite")` shows the summary of the fixed effects.
```{r TidyModSummaries, echo = TRUE}
tbls("Tab12", display = "full")
tidy(
  bmod,
  parameters = NA, 
  effects="fixed",
  robust     = FALSE, 
  conf.level = 0.95,
  conf.method = "quantile" #c("quantile", "HPDinterval")
) |> 
  mutate(
    Term = term,
    Mean = round_any(estimate,0.1), 
    SD = round_any(std.error,0.01),
    LCL = round_any(conf.low,0.1),
    UCL = round_any(conf.high,0.1),
    "Mean (SD)" = glue("{format(Mean,nsmall=1)} ({format(SD,nsmall=2)})"),
    "95% CI" = glue("({format(LCL,nsmall=1)} ;{format(UCL,nsmall=1)})")
  ) |> 
  dplyr::select(Term,`Mean (SD)`,`95% CI`) |> 
  knitr::kable() |> 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

`r tbls("Tab13", display = "cite")` shows the summary of the annual/yearly rate of change.
```{r AnnualRate, echo = TRUE}
tbls("Tab13", display = "full")
tidy(
  bmod,
  parameters = NA, 
  effects = "fixed",
  robust = FALSE, 
  conf.level = 0.95,
  conf.method = "quantile" #c("quantile", "HPDinterval")
)[3:4, ] |> 
  mutate(
    Term = term,
    Mean = round_any(estimate*12, 0.1), 
    SD = round_any(std.error*12, 0.01),
    LCL = round_any(conf.low*12, 0.1),
    UCL = round_any(conf.high*12, 0.1),
    "Annual Change (SD)" = glue("{format(Mean,nsmall=1)} ({format(SD,nsmall=2)})"),
    "95% CI" = glue("({format(LCL,nsmall=1)} ;{format(UCL,nsmall=1)})")
  ) |> 
  dplyr::select(Term, `Annual Change (SD)`, `95% CI`) |> 
  knitr::kable() |> 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```


### Predictions

```{r Predict, echo = TRUE}
# Predictions: Keeps all random effects and residual errors
ResultP <- predict(bmod, newdata = new_dataset)
head(ResultP)
# Fitted values: Drops the residual errors
ResultF <- fitted(bmod, newdata = new_dataset) 
head(ResultF)
# Fitted values less analytical variability and residual error
ResultPV <- predict(bmod, re_formula = ~ (1 | Batch), newdata = new_dataset) 
head(ResultPV)
## `Predicted data`:
pred_dataset <- cbind(new_dataset, ResultP)
head(pred_dataset)
```


`r figs("Fig2", display = "cite")` shows the predicted stability profile of 
dissolution data by batch and storage condition. The blue dashed line represent 
$Q+5$ value (85% dissolved) and the  dashed red line represent the $Q$ value 
(80% dissolved). 


```{r plt2, echo = TRUE}
ggplot(
  pred_dataset,
  aes(x = Stabtime, y = Result,  group=Batch,col = Batch)
) + 
  geom_jitter(width = 0.2,alpha = 0.1) + 
  geom_line(aes(y = Estimate)) +
  facet_grid(. ~ Condition) +  
  xlab("Stability time (months)") + 
  ylab("Dissolution value (% dissolved)") +
  geom_hline(yintercept = 85, col ="blue", linetype = "dashed") + 
  geom_hline(yintercept = 80, col = "red", linetype = "dashed" ) +   
  theme(
    axis.ticks = element_blank(),
    axis.text.x = element_text(angle = 0, hjust = 1, vjust = 0.5)
  )
figs("Fig2", display = "full")
```

### Post processing (Calculating Prob. of Failure):

`r tbls("Tab14", display = "cite")` shows the risk of failing dissolution stage testing.

```{r POF, echo = TRUE}
### Results: Probability of Failure
tbls("Tab14", display = "full")
ConStab <- cbind(
  CC = 2:3,
  expand.grid(
    Condition = levels(dataset$Condition)[2:3],
    Stabtime = c(0, 24, 36)
  )
)
J <- dim(ConStab)[1]
obj <- bmod2
postsamp <- as_draws_matrix(obj)
postsamp <- posterior_samples(obj)

Tmp <- NULL
  for(j in 1:J){
    tmpp<- tmp <- NULL
    tmpp <- round(
      100 * disso_prob(
        (postsamp[,1] + ConStab$Stabtime[j]*postsamp[,(ConStab$CC[j]+1)]),
        postsamp[,8],
        postsamp[,7],
        postsamp[,9]
      )[1:3],
      2
    )
    tmp <- data.frame(
      Condition = ConStab$Condition[j],
      Stabtime = ConStab$Stabtime[j],
      PFS1 = tmpp[1],
      PFS12 = tmpp[2],
      PFS123 = tmpp[3]
    )
    Tmp <- rbind(Tmp, tmp)
  }

POF <- Tmp |>
  filter(!((Condition == "30°C/75%RH") & (Stabtime == 0))) |>
  mutate(Condition=ifelse(Stabtime == 0, "", as.character(Condition)))
rownames(POF) <- NULL
POF <- as.data.frame(POF)
saveRDS(POF, "output/pof.rds")
POF <- readRDS("output/pof.rds")
POF %>% mutate(
  `Storage condition` = Condition,
  `Stability time` = Stabtime,
  `PF Stage I` = PFS1,
  `PF Stage II` = PFS12,
  `PF Stage III` = PFS123
) %>% 
  dplyr::select(
    `Storage condition`,
    `Stability time`,
    `PF Stage I`, 
    `PF Stage II`,
    `PF Stage III`
  ) %>% 
  knitr::kable() %>% 
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

##	Conclusion

The objective of this study is a statistical evaluation to assess the risk of 
failing dissolution stage testing (Immediate-Release Dosage Forms) at 0, 
24 and 36 months for the Active Pharmaceutical Ingredient (API) A
in batches of AB FDC FCTs packaged in blisters at dissolution time point
25 minutes and for storage conditions 25°C/60%RH and 30°C/75%RH. The 
"specification limit" or Q value is `r Q`\%.

The risk of failing dissolution stage testing was computed for storage conditions 
25°C/60%RH and 30°C/75%RH at initial, 24 and 36 months stability time points 
for dissolution time point 25 minutes and 30 minutes. There was negligible risk 
of failing all stages of dissolution stage testing which implied that there was 
negligible risk of batch rejection.

##	References{.tabset}

### Software

1. Bürkner P (2021). “Bayesian Item Response Modeling in R with brms and Stan.” Journal of Statistical Software, 100(5), 1–54. doi: 10.18637/jss.v100.i05.
2. Bürkner P (2018). “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal, 10(1), 395–411. doi: 10.32614/RJ-2018-017.
3. Bürkner P (2017). “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software, 80(1), 1–28. doi: 10.18637/jss.v080.i01.
4. Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi: 10.21105/joss.01686.
5. Plummer M, Best N, Cowles K, Vines K (2006). “CODA: Convergence Diagnosis and Output Analysis for MCMC.” R News, 6(1), 7–11. https://journal.r-project.org/archive/.

## Appendix I: R Data Listings

```{r ViewData, echo = FALSE}
datatable(dataset)
```

## Appendix II: R Session Information

```{r SessionInfo, echo = FALSE}
sessionInfo()
options(warn = 0)
```
